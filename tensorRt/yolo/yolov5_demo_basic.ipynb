{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98a8e3f",
   "metadata": {},
   "source": [
    "# So sánh việc dùng trtexec và TensorRT Python API\n",
    "\n",
    "| Tiêu chí             | `trtexec`               | TensorRT Python API                  |\n",
    "| -------------------- | ----------------------- | ------------------------------------ |\n",
    "| Dễ dùng              | ✅ rất dễ                | ❌ cần học API                        |\n",
    "| Dùng thử nhanh       | ✅                       | ❌                                    |\n",
    "| Benchmark            | ✅ rất tốt               | ✅ (phức tạp hơn)                     |\n",
    "| Production inference | ❌ không phù hợp         | ✅ chuẩn production                   |\n",
    "| Control buffer       | ❌ không có              | ✅ full control                       |\n",
    "| Dynamic batching     | ❌                       | ✅                                    |\n",
    "| Debug timing         | ✅ rất mạnh              | ✅ nhưng phải tự log                  |\n",
    "| Cài đặt              | Có sẵn khi cài TensorRT | Cần thêm pycuda, TensorRT Python SDK |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac5a5c5",
   "metadata": {},
   "source": [
    "# Chuyển đổi một model object detection YOLOv5 từ ONNX sang TensorRT, build engine, optimize batch size, và benchmark tốc độ inference.\n",
    "\n",
    "- Trong bài toán này, sẽ đụng đến:\n",
    "    - Dynamic shapes (batch size động)\n",
    "    - Workspace size (GPU memory allocation cho engine builder)\n",
    "    - FP16 optimization\n",
    "    - TensorRT calibration cho INT8 (mình sẽ giới thiệu, nhưng chưa bắt buộc code full calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85534cd1",
   "metadata": {},
   "source": [
    "# Import các thư viện ở package khác"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56b71fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Thêm thư mục gốc project (/app) vào sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# Sau đó import\n",
    "from util.util import *\n",
    "from util.config import *\n",
    "import trt_infer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30698466",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11c8b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = \"yolov5s.onnx\"\n",
    "engine_path = \"yolov5s.engine\"\n",
    "img_folder_path = './data/images'\n",
    "output_folder_path = 'output_results_trt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2027fbd",
   "metadata": {},
   "source": [
    "## 1️⃣ Chuẩn bị model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f98c2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.24.4 in /usr/local/lib/python3.10/dist-packages (1.24.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: onnx==1.14.1 in /usr/local/lib/python3.10/dist-packages (1.14.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx==1.14.1) (1.24.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx==1.14.1) (6.31.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx==1.14.1) (4.10.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Hit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "91 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "libgl1 is already the newest version (1.4.0-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 91 not upgraded.\n",
      "OpenCV version: 4.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.24.4\n",
    "!pip install onnx==1.14.1 --upgrade\n",
    "!apt update\n",
    "!apt install -y libgl1\n",
    "\n",
    "import cv2\n",
    "print(\"OpenCV version:\", cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d3aae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.1.44)\n",
      "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.10.3)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.24.4)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (10.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (7.0.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2.32.4)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.15.3)\n",
      "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (1.11.0+cpu)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (0.12.0+cpu)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (4.67.1)\n",
      "Requirement already satisfied: ultralytics>=8.2.34 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (8.3.155)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (2.3.0)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (0.13.2)\n",
      "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 42)) (80.9.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython>=3.1.30->-r requirements.txt (line 5)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5)) (5.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2025.6.15)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (4.10.0)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (9.0.0)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (2.0.14)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mWARNING ⚠️ user config directory '/root/.config/Ultralytics' is not writeable, defaulting to '/tmp' or CWD.Alternatively you can define a YOLO_CONFIG_DIR environment variable for this path.\n",
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/tmp/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "/app/tensorRt/yolo/yolov5/utils/general.py:32: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources as pkg\n",
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['yolov5s.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, per_tensor=False, dynamic=False, cache=, simplify=False, mlmodel=False, opset=17, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
      "fatal: detected dubious ownership in repository at '/app/tensorRt/yolo/yolov5'\n",
      "To add an exception for this directory, call:\n",
      "\n",
      "\tgit config --global --add safe.directory /app/tensorRt/yolo/yolov5\n",
      "YOLOv5 🚀 2025-6-15 Python-3.10.12 torch-1.11.0+cpu CPU\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|██████████████████████████████████████| 14.1M/14.1M [00:00<00:00, 34.2MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from yolov5s.pt with output shape (1, 25200, 85) (14.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export failure ❌ 0.0s: Unsupported ONNX opset version: 17\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5\n",
    "# %cd yolov5\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Export model sang ONNX\n",
    "!python export.py --weights yolov5s.pt --include onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3c2e5c",
   "metadata": {},
   "source": [
    "**Nếu gặp lỗi**\n",
    "```\n",
    "ONNX: starting export with onnx 1.14.1...\n",
    "ONNX: export failure ❌ 0.0s: Unsupported ONNX opset version: 17\n",
    "```\n",
    "- Giải thích:\n",
    "    - YOLOv5 mặc định hiện tại export sang ONNX với opset=17\n",
    "    - Nhưng onnx version trong môi trường đang dùng chỉ hỗ trợ tối đa opset 16  \n",
    "- Giải quyết có 2 cách:\n",
    "    - Cách 1:\n",
    "        -  Xuất ONNX với opset thấp hơn (an toàn nhất) \n",
    "    - Cách 2:\n",
    "        - Cập nhật phiên bản ONNX lên 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81b0eef",
   "metadata": {},
   "source": [
    "- Vì sao hạ opset_version thường an toàn hơn?\n",
    "    - Tính tương thích (compatibility)\n",
    "        - ONNX opset định nghĩa các phép toán (ops) tại từng phiên bản.\n",
    "        - Các exporter (như torch.onnx.export() hay tf2onnx) chuyển các hàm thành toán tử ONNX tương ứng.\n",
    "        - Opset mới có thể chưa được onnxruntime hay các engine inference khác hỗ trợ đầy đủ ⇒ dễ gặp lỗi khi chạy (unsupported ops, runtime errors…).\n",
    "        - Hạ opset_version giúp dùng tập toán tử ổn định, đã được hỗ trợ rộng rãi trên nhiều nền tảng.\n",
    "        - Giảm thiểu khả năng gặp lỗi \"Unsupported operator\" khi inference ở production, cloud, mobile, hay các thiết bị nhúng.\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34c7e6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/tensorRt/yolo/yolov5/utils/general.py:32: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources as pkg\n",
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['yolov5s.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, per_tensor=False, dynamic=False, cache=, simplify=False, mlmodel=False, opset=15, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
      "fatal: detected dubious ownership in repository at '/app/tensorRt/yolo/yolov5'\n",
      "To add an exception for this directory, call:\n",
      "\n",
      "\tgit config --global --add safe.directory /app/tensorRt/yolo/yolov5\n",
      "YOLOv5 🚀 2025-6-15 Python-3.10.12 torch-1.11.0+cpu CPU\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|███████████████████████████████████████| 14.1M/14.1M [00:27<00:00, 546kB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from yolov5s.pt with output shape (1, 25200, 85) (14.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1...\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 1.7s, saved as yolov5s.onnx (28.0 MB)\n",
      "\n",
      "Export complete (30.5s)\n",
      "Results saved to \u001b[1m/app/tensorRt/yolo/yolov5\u001b[0m\n",
      "Detect:          python detect.py --weights yolov5s.onnx \n",
      "Validate:        python val.py --weights yolov5s.onnx \n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5s.onnx')  \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "# Hạ xuống opset version 16\n",
    "!python export.py --weights yolov5s.pt --include onnx --opset 15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cc3c3c",
   "metadata": {},
   "source": [
    "# 2️⃣ Build TensorRT Engine Với Dynamic Shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72755f",
   "metadata": {},
   "source": [
    "## Dùng trtexec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4549f1",
   "metadata": {},
   "source": [
    "### 1️⃣ Kiểm tra file ONNX hợp lệ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692975dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ONNX model hợp lệ!\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import os\n",
    "\n",
    "# Kiểm tra file có tồn tại không\n",
    "if not os.path.isfile(onnx_path):\n",
    "    raise FileNotFoundError(f\"Không tìm thấy file {onnx_path}\")\n",
    "\n",
    "# Load và kiểm tra model\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"✅ ONNX model hợp lệ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940dc535",
   "metadata": {},
   "source": [
    "### 2️⃣ Kiểm tra trtexec có sẵn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ced1d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/tensorrt/bin/trtexec\n"
     ]
    }
   ],
   "source": [
    "!which trtexec || echo \"⚠️ trtexec chưa được cài đặt hoặc không nằm trong PATH.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45d6f27",
   "metadata": {},
   "source": [
    "### 3️⃣ Convert ONNX sang TensorRT Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d271d117",
   "metadata": {},
   "source": [
    "#### Kiểm tra input name trong file onnx để truyền vào minShapes, optShapes, maxShapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c16d456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT NAME: images\n"
     ]
    }
   ],
   "source": [
    "model = onnx.load(onnx_path)\n",
    "for input in model.graph.input:\n",
    "    print(\"INPUT NAME: \" + input.name) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30abcc01",
   "metadata": {},
   "source": [
    "#### kiểm tra INPUT của ONNX model là static hay dynamic shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593eced2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model inputs của yolov5s.onnx:\n",
      "- images: [1, 3, 640, 640] --> STATIC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'images': {'shape': [1, 3, 640, 640], 'dynamic': False}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_onnx_input_shapes(onnx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6372d4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8603] # trtexec --onnx=yolov5s.onnx --saveEngine=yolov5s.engine --explicitBatch --workspace=4096 --fp16\n",
      "[06/15/2025-09:50:31] [W] --explicitBatch flag has been deprecated and has no effect!\n",
      "[06/15/2025-09:50:31] [W] Explicit batch dim is automatically enabled if input model is ONNX or if dynamic shapes are provided when the engine is built.\n",
      "[06/15/2025-09:50:31] [W] --workspace flag has been deprecated by --memPoolSize flag.\n",
      "[06/15/2025-09:50:31] [I] === Model Options ===\n",
      "[06/15/2025-09:50:31] [I] Format: ONNX\n",
      "[06/15/2025-09:50:31] [I] Model: yolov5s.onnx\n",
      "[06/15/2025-09:50:31] [I] Output:\n",
      "[06/15/2025-09:50:31] [I] === Build Options ===\n",
      "[06/15/2025-09:50:31] [I] Max batch: explicit batch\n",
      "[06/15/2025-09:50:31] [I] Memory Pools: workspace: 4096 MiB, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
      "[06/15/2025-09:50:31] [I] minTiming: 1\n",
      "[06/15/2025-09:50:31] [I] avgTiming: 8\n",
      "[06/15/2025-09:50:31] [I] Precision: FP32+FP16\n",
      "[06/15/2025-09:50:31] [I] LayerPrecisions: \n",
      "[06/15/2025-09:50:31] [I] Layer Device Types: \n",
      "[06/15/2025-09:50:31] [I] Calibration: \n",
      "[06/15/2025-09:50:31] [I] Refit: Disabled\n",
      "[06/15/2025-09:50:31] [I] Version Compatible: Disabled\n",
      "[06/15/2025-09:50:31] [I] ONNX Native InstanceNorm: Disabled\n",
      "[06/15/2025-09:50:31] [I] TensorRT runtime: full\n",
      "[06/15/2025-09:50:31] [I] Lean DLL Path: \n",
      "[06/15/2025-09:50:31] [I] Tempfile Controls: { in_memory: allow, temporary: allow }\n",
      "[06/15/2025-09:50:31] [I] Exclude Lean Runtime: Disabled\n",
      "[06/15/2025-09:50:31] [I] Sparsity: Disabled\n",
      "[06/15/2025-09:50:31] [I] Safe mode: Disabled\n",
      "[06/15/2025-09:50:31] [I] Build DLA standalone loadable: Disabled\n",
      "[06/15/2025-09:50:31] [I] Allow GPU fallback for DLA: Disabled\n",
      "[06/15/2025-09:50:31] [I] DirectIO mode: Disabled\n",
      "[06/15/2025-09:50:31] [I] Restricted mode: Disabled\n",
      "[06/15/2025-09:50:31] [I] Skip inference: Disabled\n",
      "[06/15/2025-09:50:31] [I] Save engine: yolov5s.engine\n",
      "[06/15/2025-09:50:31] [I] Load engine: \n",
      "[06/15/2025-09:50:31] [I] Profiling verbosity: 0\n",
      "[06/15/2025-09:50:31] [I] Tactic sources: Using default tactic sources\n",
      "[06/15/2025-09:50:31] [I] timingCacheMode: local\n",
      "[06/15/2025-09:50:31] [I] timingCacheFile: \n",
      "[06/15/2025-09:50:31] [I] Heuristic: Disabled\n",
      "[06/15/2025-09:50:31] [I] Preview Features: Use default preview flags.\n",
      "[06/15/2025-09:50:31] [I] MaxAuxStreams: -1\n",
      "[06/15/2025-09:50:31] [I] BuilderOptimizationLevel: -1\n",
      "[06/15/2025-09:50:31] [I] Input(s)s format: fp32:CHW\n",
      "[06/15/2025-09:50:31] [I] Output(s)s format: fp32:CHW\n",
      "[06/15/2025-09:50:31] [I] Input build shapes: model\n",
      "[06/15/2025-09:50:31] [I] Input calibration shapes: model\n",
      "[06/15/2025-09:50:31] [I] === System Options ===\n",
      "[06/15/2025-09:50:31] [I] Device: 0\n",
      "[06/15/2025-09:50:31] [I] DLACore: \n",
      "[06/15/2025-09:50:31] [I] Plugins:\n",
      "[06/15/2025-09:50:31] [I] setPluginsToSerialize:\n",
      "[06/15/2025-09:50:31] [I] dynamicPlugins:\n",
      "[06/15/2025-09:50:31] [I] ignoreParsedPluginLibs: 0\n",
      "[06/15/2025-09:50:31] [I] \n",
      "[06/15/2025-09:50:31] [I] === Inference Options ===\n",
      "[06/15/2025-09:50:31] [I] Batch: Explicit\n",
      "[06/15/2025-09:50:31] [I] Input inference shapes: model\n",
      "[06/15/2025-09:50:31] [I] Iterations: 10\n",
      "[06/15/2025-09:50:31] [I] Duration: 3s (+ 200ms warm up)\n",
      "[06/15/2025-09:50:31] [I] Sleep time: 0ms\n",
      "[06/15/2025-09:50:31] [I] Idle time: 0ms\n",
      "[06/15/2025-09:50:31] [I] Inference Streams: 1\n",
      "[06/15/2025-09:50:31] [I] ExposeDMA: Disabled\n",
      "[06/15/2025-09:50:31] [I] Data transfers: Enabled\n",
      "[06/15/2025-09:50:31] [I] Spin-wait: Disabled\n",
      "[06/15/2025-09:50:31] [I] Multithreading: Disabled\n",
      "[06/15/2025-09:50:31] [I] CUDA Graph: Disabled\n",
      "[06/15/2025-09:50:31] [I] Separate profiling: Disabled\n",
      "[06/15/2025-09:50:31] [I] Time Deserialize: Disabled\n",
      "[06/15/2025-09:50:31] [I] Time Refit: Disabled\n",
      "[06/15/2025-09:50:31] [I] NVTX verbosity: 0\n",
      "[06/15/2025-09:50:31] [I] Persistent Cache Ratio: 0\n",
      "[06/15/2025-09:50:31] [I] Inputs:\n",
      "[06/15/2025-09:50:31] [I] === Reporting Options ===\n",
      "[06/15/2025-09:50:31] [I] Verbose: Disabled\n",
      "[06/15/2025-09:50:31] [I] Averages: 10 inferences\n",
      "[06/15/2025-09:50:31] [I] Percentiles: 90,95,99\n",
      "[06/15/2025-09:50:31] [I] Dump refittable layers:Disabled\n",
      "[06/15/2025-09:50:31] [I] Dump output: Disabled\n",
      "[06/15/2025-09:50:31] [I] Profile: Disabled\n",
      "[06/15/2025-09:50:31] [I] Export timing to JSON file: \n",
      "[06/15/2025-09:50:31] [I] Export output to JSON file: \n",
      "[06/15/2025-09:50:31] [I] Export profile to JSON file: \n",
      "[06/15/2025-09:50:31] [I] \n",
      "[06/15/2025-09:50:32] [I] === Device Information ===\n",
      "[06/15/2025-09:50:32] [I] Selected Device: NVIDIA GeForce RTX 2060\n",
      "[06/15/2025-09:50:32] [I] Compute Capability: 7.5\n",
      "[06/15/2025-09:50:32] [I] SMs: 30\n",
      "[06/15/2025-09:50:32] [I] Device Global Memory: 6143 MiB\n",
      "[06/15/2025-09:50:32] [I] Shared Memory per SM: 64 KiB\n",
      "[06/15/2025-09:50:32] [I] Memory Bus Width: 192 bits (ECC disabled)\n",
      "[06/15/2025-09:50:32] [I] Application Compute Clock Rate: 1.71 GHz\n",
      "[06/15/2025-09:50:32] [I] Application Memory Clock Rate: 7.001 GHz\n",
      "[06/15/2025-09:50:32] [I] \n",
      "[06/15/2025-09:50:32] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.\n",
      "[06/15/2025-09:50:32] [I] \n",
      "[06/15/2025-09:50:32] [I] TensorRT version: 8.6.3\n",
      "[06/15/2025-09:50:32] [I] Loading standard plugins\n",
      "[06/15/2025-09:50:32] [I] [TRT] [MemUsageChange] Init CUDA: CPU +506, GPU +0, now: CPU 523, GPU 1093 (MiB)\n",
      "[06/15/2025-09:50:37] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +884, GPU +172, now: CPU 1483, GPU 1265 (MiB)\n",
      "[06/15/2025-09:50:37] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "[06/15/2025-09:50:37] [I] Start parsing network model.\n",
      "[06/15/2025-09:50:37] [I] [TRT] ----------------------------------------------------------------\n",
      "[06/15/2025-09:50:37] [I] [TRT] Input filename:   yolov5s.onnx\n",
      "[06/15/2025-09:50:37] [I] [TRT] ONNX IR version:  0.0.8\n",
      "[06/15/2025-09:50:37] [I] [TRT] Opset version:    15\n",
      "[06/15/2025-09:50:37] [I] [TRT] Producer name:    pytorch\n",
      "[06/15/2025-09:50:37] [I] [TRT] Producer version: 1.11.0\n",
      "[06/15/2025-09:50:37] [I] [TRT] Domain:           \n",
      "[06/15/2025-09:50:37] [I] [TRT] Model version:    0\n",
      "[06/15/2025-09:50:37] [I] [TRT] Doc string:       \n",
      "[06/15/2025-09:50:37] [I] [TRT] ----------------------------------------------------------------\n",
      "[06/15/2025-09:50:37] [W] [TRT] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[06/15/2025-09:50:37] [I] Finished parsing network model. Parse time: 0.363563\n",
      "[06/15/2025-09:50:37] [I] [TRT] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[06/15/2025-09:50:37] [I] [TRT] Graph optimization time: 0.0245845 seconds.\n",
      "[06/15/2025-09:50:37] [I] [TRT] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[06/15/2025-09:50:37] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[06/15/2025-09:55:31] [I] [TRT] Detected 1 inputs and 4 output network tensors.\n",
      "[06/15/2025-09:55:31] [I] [TRT] Total Host Persistent Memory: 266768\n",
      "[06/15/2025-09:55:31] [I] [TRT] Total Device Persistent Memory: 718336\n",
      "[06/15/2025-09:55:31] [I] [TRT] Total Scratch Memory: 512\n",
      "[06/15/2025-09:55:31] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 17 MiB, GPU 213 MiB\n",
      "[06/15/2025-09:55:31] [I] [TRT] [BlockAssignment] Started assigning block shifts. This will take 103 steps to complete.\n",
      "[06/15/2025-09:55:31] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 4.0053ms to assign 11 blocks to 103 nodes requiring 17066496 bytes.\n",
      "[06/15/2025-09:55:31] [I] [TRT] Total Activation Memory: 17066496\n",
      "[06/15/2025-09:55:31] [W] [TRT] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[06/15/2025-09:55:31] [W] [TRT] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[06/15/2025-09:55:31] [W] [TRT] Check verbose logs for the list of affected weights.\n",
      "[06/15/2025-09:55:31] [W] [TRT] - 52 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "[06/15/2025-09:55:31] [W] [TRT] - 2 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n",
      "[06/15/2025-09:55:31] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +13, GPU +15, now: CPU 13, GPU 15 (MiB)\n",
      "[06/15/2025-09:55:31] [I] Engine built in 299.401 sec.\n",
      "[06/15/2025-09:55:31] [I] [TRT] Loaded engine size: 15 MiB\n",
      "[06/15/2025-09:55:31] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +14, now: CPU 0, GPU 14 (MiB)\n",
      "[06/15/2025-09:55:31] [I] Engine deserialized in 0.0304019 sec.\n",
      "[06/15/2025-09:55:31] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +17, now: CPU 0, GPU 31 (MiB)\n",
      "[06/15/2025-09:55:31] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "[06/15/2025-09:55:31] [I] Setting persistentCacheLimit to 0 bytes.\n",
      "[06/15/2025-09:55:31] [I] Using random values for input images\n",
      "[06/15/2025-09:55:31] [I] Input binding for images with dimensions 1x3x640x640 is created.\n",
      "[06/15/2025-09:55:31] [I] Output binding for output0 with dimensions 1x25200x85 is created.\n",
      "[06/15/2025-09:55:31] [I] Starting inference\n",
      "[06/15/2025-09:55:34] [I] Warmup completed 83 queries over 200 ms\n",
      "[06/15/2025-09:55:34] [I] Timing trace has 1245 queries over 3.00621 s\n",
      "[06/15/2025-09:55:34] [I] \n",
      "[06/15/2025-09:55:34] [I] === Trace details ===\n",
      "[06/15/2025-09:55:34] [I] Trace averages of 10 runs:\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.81064 ms - Host latency: 3.11746 ms (enqueue 0.3097 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.72516 ms - Host latency: 3.03961 ms (enqueue 0.343556 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.82008 ms - Host latency: 3.20615 ms (enqueue 0.335641 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.9661 ms - Host latency: 3.85549 ms (enqueue 1.20757 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.82114 ms - Host latency: 3.81736 ms (enqueue 0.895084 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.83547 ms - Host latency: 3.17896 ms (enqueue 0.349002 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74182 ms - Host latency: 3.05858 ms (enqueue 0.334525 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.81488 ms - Host latency: 3.30452 ms (enqueue 0.359946 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.79232 ms - Host latency: 3.27472 ms (enqueue 0.396158 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.79866 ms - Host latency: 3.14948 ms (enqueue 0.324313 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.71331 ms - Host latency: 3.01493 ms (enqueue 0.304929 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.81517 ms - Host latency: 3.17927 ms (enqueue 0.376053 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.77389 ms - Host latency: 3.08079 ms (enqueue 0.391074 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.78853 ms - Host latency: 3.16204 ms (enqueue 0.516663 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.76529 ms - Host latency: 3.05678 ms (enqueue 0.391846 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.71794 ms - Host latency: 3.05901 ms (enqueue 0.335925 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.75856 ms - Host latency: 3.10734 ms (enqueue 0.376563 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74085 ms - Host latency: 3.04981 ms (enqueue 0.304736 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.75717 ms - Host latency: 3.05554 ms (enqueue 0.313324 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.6922 ms - Host latency: 3.03969 ms (enqueue 0.31955 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.75135 ms - Host latency: 3.09667 ms (enqueue 0.347931 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.71611 ms - Host latency: 3.03034 ms (enqueue 0.340344 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.71002 ms - Host latency: 3.02034 ms (enqueue 0.303632 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.73773 ms - Host latency: 3.08653 ms (enqueue 0.352832 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.75787 ms - Host latency: 3.0756 ms (enqueue 0.324133 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74556 ms - Host latency: 3.07076 ms (enqueue 0.333405 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.72435 ms - Host latency: 3.05628 ms (enqueue 0.316302 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.7718 ms - Host latency: 3.10431 ms (enqueue 0.343378 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.73354 ms - Host latency: 3.03343 ms (enqueue 0.328699 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.704 ms - Host latency: 3.01304 ms (enqueue 0.304883 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.7913 ms - Host latency: 3.23281 ms (enqueue 0.385053 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.75477 ms - Host latency: 3.08052 ms (enqueue 0.338086 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.80508 ms - Host latency: 3.12257 ms (enqueue 0.341986 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74809 ms - Host latency: 3.03586 ms (enqueue 0.32749 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.79978 ms - Host latency: 3.24683 ms (enqueue 0.475549 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.69916 ms - Host latency: 3.07426 ms (enqueue 0.339819 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.80101 ms - Host latency: 3.20576 ms (enqueue 0.365527 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.75115 ms - Host latency: 3.23462 ms (enqueue 0.520825 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.81997 ms - Host latency: 3.15333 ms (enqueue 0.5073 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.79821 ms - Host latency: 3.36172 ms (enqueue 0.544897 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.863 ms - Host latency: 3.30674 ms (enqueue 0.40592 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.71931 ms - Host latency: 3.11305 ms (enqueue 0.352502 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.82367 ms - Host latency: 3.21552 ms (enqueue 0.365234 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.71818 ms - Host latency: 3.05835 ms (enqueue 0.319714 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.78802 ms - Host latency: 3.14032 ms (enqueue 0.481897 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.75718 ms - Host latency: 3.0632 ms (enqueue 0.530042 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.6964 ms - Host latency: 3.04036 ms (enqueue 0.525073 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.78032 ms - Host latency: 3.1306 ms (enqueue 0.493713 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74441 ms - Host latency: 3.05566 ms (enqueue 0.322571 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.7438 ms - Host latency: 3.04594 ms (enqueue 0.339502 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.70337 ms - Host latency: 3.04087 ms (enqueue 0.305103 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74395 ms - Host latency: 3.06234 ms (enqueue 0.319751 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.71417 ms - Host latency: 3.02804 ms (enqueue 0.332544 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.70452 ms - Host latency: 3.02292 ms (enqueue 0.307776 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74871 ms - Host latency: 3.0608 ms (enqueue 0.318591 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74635 ms - Host latency: 3.04088 ms (enqueue 0.307214 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.72532 ms - Host latency: 3.15673 ms (enqueue 0.334094 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.6889 ms - Host latency: 3.01816 ms (enqueue 0.319238 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.7803 ms - Host latency: 3.18451 ms (enqueue 0.391785 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.7448 ms - Host latency: 3.03251 ms (enqueue 0.308557 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.77694 ms - Host latency: 3.09116 ms (enqueue 0.311267 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74636 ms - Host latency: 3.12823 ms (enqueue 0.396106 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.7377 ms - Host latency: 3.03025 ms (enqueue 0.314209 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.80168 ms - Host latency: 3.10912 ms (enqueue 0.305432 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.73159 ms - Host latency: 3.01456 ms (enqueue 0.299329 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.80736 ms - Host latency: 3.12594 ms (enqueue 0.32821 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.76212 ms - Host latency: 3.07501 ms (enqueue 0.379968 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.7869 ms - Host latency: 3.07871 ms (enqueue 0.319409 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.73882 ms - Host latency: 3.04816 ms (enqueue 0.313354 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74962 ms - Host latency: 3.15979 ms (enqueue 0.322852 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74749 ms - Host latency: 3.26335 ms (enqueue 0.404321 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.75977 ms - Host latency: 3.10012 ms (enqueue 0.355261 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74971 ms - Host latency: 3.05521 ms (enqueue 0.329993 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.68951 ms - Host latency: 3.03015 ms (enqueue 0.319006 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74332 ms - Host latency: 3.07635 ms (enqueue 0.309741 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.76907 ms - Host latency: 3.05215 ms (enqueue 0.314465 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.72048 ms - Host latency: 3.03398 ms (enqueue 0.328467 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.72769 ms - Host latency: 3.09585 ms (enqueue 0.324854 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.75071 ms - Host latency: 3.07644 ms (enqueue 0.31814 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.72129 ms - Host latency: 3.04856 ms (enqueue 0.34353 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.6915 ms - Host latency: 3.02241 ms (enqueue 0.337451 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74507 ms - Host latency: 3.04578 ms (enqueue 0.319336 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.75127 ms - Host latency: 3.03499 ms (enqueue 0.303882 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.6895 ms - Host latency: 2.99978 ms (enqueue 0.303589 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.75339 ms - Host latency: 3.12124 ms (enqueue 0.375757 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.75303 ms - Host latency: 3.07202 ms (enqueue 0.305566 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74419 ms - Host latency: 3.07412 ms (enqueue 0.328223 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.72214 ms - Host latency: 3.06216 ms (enqueue 0.340454 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74597 ms - Host latency: 3.08979 ms (enqueue 0.344287 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.77207 ms - Host latency: 3.06829 ms (enqueue 0.309033 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.70317 ms - Host latency: 3.02263 ms (enqueue 0.320752 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.77776 ms - Host latency: 3.10564 ms (enqueue 0.33335 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.77056 ms - Host latency: 3.29592 ms (enqueue 0.391626 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.77415 ms - Host latency: 3.31047 ms (enqueue 0.635352 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.78552 ms - Host latency: 3.24392 ms (enqueue 0.464526 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.70679 ms - Host latency: 3.03726 ms (enqueue 0.321704 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.7509 ms - Host latency: 3.05469 ms (enqueue 0.313672 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.73135 ms - Host latency: 3.12322 ms (enqueue 0.380786 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.69529 ms - Host latency: 3.01367 ms (enqueue 0.324365 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74431 ms - Host latency: 3.07542 ms (enqueue 0.340747 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74312 ms - Host latency: 3.04172 ms (enqueue 0.365039 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.68604 ms - Host latency: 3.00562 ms (enqueue 0.351025 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.78503 ms - Host latency: 3.14216 ms (enqueue 0.365796 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74041 ms - Host latency: 3.05916 ms (enqueue 0.328906 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74785 ms - Host latency: 3.03511 ms (enqueue 0.30791 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.69141 ms - Host latency: 3.00386 ms (enqueue 0.321338 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.76233 ms - Host latency: 3.0863 ms (enqueue 0.326392 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.71565 ms - Host latency: 3.03533 ms (enqueue 0.348999 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.7145 ms - Host latency: 3.02988 ms (enqueue 0.317456 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.73091 ms - Host latency: 3.08059 ms (enqueue 0.350269 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74773 ms - Host latency: 3.05825 ms (enqueue 0.321094 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.75396 ms - Host latency: 3.05181 ms (enqueue 0.305859 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.72136 ms - Host latency: 3.05679 ms (enqueue 0.358789 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.77151 ms - Host latency: 3.1323 ms (enqueue 0.362646 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74292 ms - Host latency: 3.05447 ms (enqueue 0.344727 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.73511 ms - Host latency: 3.07222 ms (enqueue 0.318774 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.73738 ms - Host latency: 3.09453 ms (enqueue 0.347021 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.74104 ms - Host latency: 3.04336 ms (enqueue 0.311353 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.76316 ms - Host latency: 3.06174 ms (enqueue 0.325732 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.69033 ms - Host latency: 3.02727 ms (enqueue 0.313794 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.76106 ms - Host latency: 3.10039 ms (enqueue 0.335278 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.71846 ms - Host latency: 3.03909 ms (enqueue 0.382617 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.7145 ms - Host latency: 3.06104 ms (enqueue 0.383057 ms)\n",
      "[06/15/2025-09:55:34] [I] Average on 10 runs - GPU latency: 1.71958 ms - Host latency: 3.03628 ms (enqueue 0.371533 ms)\n",
      "[06/15/2025-09:55:34] [I] \n",
      "[06/15/2025-09:55:34] [I] === Performance summary ===\n",
      "[06/15/2025-09:55:34] [I] Throughput: 414.143 qps\n",
      "[06/15/2025-09:55:34] [I] Latency: min = 2.94824 ms, max = 4.715 ms, mean = 3.10404 ms, median = 2.99646 ms, percentile(90%) = 3.41553 ms, percentile(95%) = 3.56226 ms, percentile(99%) = 3.90234 ms\n",
      "[06/15/2025-09:55:34] [I] Enqueue Time: min = 0.266846 ms, max = 2.0961 ms, mean = 0.364264 ms, median = 0.315735 ms, percentile(90%) = 0.51001 ms, percentile(95%) = 0.588013 ms, percentile(99%) = 0.947784 ms\n",
      "[06/15/2025-09:55:34] [I] H2D Latency: min = 0.393311 ms, max = 1.36417 ms, mean = 0.533244 ms, median = 0.615234 ms, percentile(90%) = 0.681427 ms, percentile(95%) = 0.747559 ms, percentile(99%) = 0.917664 ms\n",
      "[06/15/2025-09:55:34] [I] GPU Compute Time: min = 1.65967 ms, max = 2.59781 ms, mean = 1.75153 ms, median = 1.70337 ms, percentile(90%) = 1.97876 ms, percentile(95%) = 2.16162 ms, percentile(99%) = 2.30371 ms\n",
      "[06/15/2025-09:55:34] [I] D2H Latency: min = 0.650146 ms, max = 1.89719 ms, mean = 0.819269 ms, median = 0.890808 ms, percentile(90%) = 0.98999 ms, percentile(95%) = 1.09937 ms, percentile(99%) = 1.3403 ms\n",
      "[06/15/2025-09:55:34] [I] Total Host Walltime: 3.00621 s\n",
      "[06/15/2025-09:55:34] [I] Total GPU Compute Time: 2.18065 s\n",
      "[06/15/2025-09:55:34] [W] * GPU compute time is unstable, with coefficient of variance = 8.63391%.\n",
      "[06/15/2025-09:55:34] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n",
      "[06/15/2025-09:55:34] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[06/15/2025-09:55:34] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8603] # trtexec --onnx=yolov5s.onnx --saveEngine=yolov5s.engine --explicitBatch --workspace=4096 --fp16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --onnx: Chỉ định file onnx.\n",
    "# --saveEngine: Đầu ra file engine.\n",
    "# --explicitBatch: Bắt buộc khi dùng TensorRT 7+ với dynamic shape.\n",
    "# minShapes, optShapes, maxShapes: Giới hạn phạm vi batch size engine có thể tối ưu (từ 1 đến 8).\n",
    "# --workspace: Cấp phát RAM GPU tối đa 4GB cho quá trình build engine.\n",
    "# --fp16: Bật mixed precision (half precision)(FP16), tăng tốc độ nhưng vẫn giữ độ chính xác khá cao.\n",
    "# Có thể thêm --int8 nếu đã calibrate mô hình.\n",
    "\n",
    "'''\n",
    "Dynamic input shape\n",
    "'''\n",
    "# cmd = \"\"\"\n",
    "# trtexec \\\n",
    "#   --onnx=yolov5s.onnx \\\n",
    "#   --saveEngine=yolov5s.engine \\\n",
    "#   --explicitBatch \\\n",
    "#   --minShapes=images:1x3x640x640 \\\n",
    "#   --optShapes=images:4x3x640x640 \\\n",
    "#   --maxShapes=images:8x3x640x640 \\\n",
    "#   --workspace=4096 \\\n",
    "#   --fp16\n",
    "# \"\"\"\n",
    "# !$cmd\n",
    "\n",
    "cmd = \"\"\"\n",
    "trtexec \\\n",
    "  --onnx=yolov5s.onnx \\\n",
    "  --saveEngine=yolov5s.engine \\\n",
    "  --explicitBatch \\\n",
    "  --workspace=4096 \\\n",
    "  --fp16\n",
    "\"\"\"\n",
    "!$cmd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc2a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TensorRT engine đã được tạo thành công!\n"
     ]
    }
   ],
   "source": [
    "### 4️⃣ Kiểm tra file engine sau khi build\n",
    "if os.path.isfile(engine_path):\n",
    "    print(\"✅ TensorRT engine đã được tạo thành công!\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"❌ TensorRT engine chưa được tạo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9ecbadb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8603] # trtexec --loadEngine=yolov5s.engine --iterations=100\n",
      "[06/15/2025-10:08:37] [I] === Model Options ===\n",
      "[06/15/2025-10:08:37] [I] Format: *\n",
      "[06/15/2025-10:08:37] [I] Model: \n",
      "[06/15/2025-10:08:37] [I] Output:\n",
      "[06/15/2025-10:08:37] [I] === Build Options ===\n",
      "[06/15/2025-10:08:37] [I] Max batch: 1\n",
      "[06/15/2025-10:08:37] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
      "[06/15/2025-10:08:37] [I] minTiming: 1\n",
      "[06/15/2025-10:08:37] [I] avgTiming: 8\n",
      "[06/15/2025-10:08:37] [I] Precision: FP32\n",
      "[06/15/2025-10:08:37] [I] LayerPrecisions: \n",
      "[06/15/2025-10:08:37] [I] Layer Device Types: \n",
      "[06/15/2025-10:08:37] [I] Calibration: \n",
      "[06/15/2025-10:08:37] [I] Refit: Disabled\n",
      "[06/15/2025-10:08:37] [I] Version Compatible: Disabled\n",
      "[06/15/2025-10:08:37] [I] ONNX Native InstanceNorm: Disabled\n",
      "[06/15/2025-10:08:37] [I] TensorRT runtime: full\n",
      "[06/15/2025-10:08:37] [I] Lean DLL Path: \n",
      "[06/15/2025-10:08:37] [I] Tempfile Controls: { in_memory: allow, temporary: allow }\n",
      "[06/15/2025-10:08:37] [I] Exclude Lean Runtime: Disabled\n",
      "[06/15/2025-10:08:37] [I] Sparsity: Disabled\n",
      "[06/15/2025-10:08:37] [I] Safe mode: Disabled\n",
      "[06/15/2025-10:08:37] [I] Build DLA standalone loadable: Disabled\n",
      "[06/15/2025-10:08:37] [I] Allow GPU fallback for DLA: Disabled\n",
      "[06/15/2025-10:08:37] [I] DirectIO mode: Disabled\n",
      "[06/15/2025-10:08:37] [I] Restricted mode: Disabled\n",
      "[06/15/2025-10:08:37] [I] Skip inference: Disabled\n",
      "[06/15/2025-10:08:37] [I] Save engine: \n",
      "[06/15/2025-10:08:37] [I] Load engine: yolov5s.engine\n",
      "[06/15/2025-10:08:37] [I] Profiling verbosity: 0\n",
      "[06/15/2025-10:08:37] [I] Tactic sources: Using default tactic sources\n",
      "[06/15/2025-10:08:37] [I] timingCacheMode: local\n",
      "[06/15/2025-10:08:37] [I] timingCacheFile: \n",
      "[06/15/2025-10:08:37] [I] Heuristic: Disabled\n",
      "[06/15/2025-10:08:37] [I] Preview Features: Use default preview flags.\n",
      "[06/15/2025-10:08:37] [I] MaxAuxStreams: -1\n",
      "[06/15/2025-10:08:37] [I] BuilderOptimizationLevel: -1\n",
      "[06/15/2025-10:08:37] [I] Input(s)s format: fp32:CHW\n",
      "[06/15/2025-10:08:37] [I] Output(s)s format: fp32:CHW\n",
      "[06/15/2025-10:08:37] [I] Input build shapes: model\n",
      "[06/15/2025-10:08:37] [I] Input calibration shapes: model\n",
      "[06/15/2025-10:08:37] [I] === System Options ===\n",
      "[06/15/2025-10:08:37] [I] Device: 0\n",
      "[06/15/2025-10:08:37] [I] DLACore: \n",
      "[06/15/2025-10:08:37] [I] Plugins:\n",
      "[06/15/2025-10:08:37] [I] setPluginsToSerialize:\n",
      "[06/15/2025-10:08:37] [I] dynamicPlugins:\n",
      "[06/15/2025-10:08:37] [I] ignoreParsedPluginLibs: 0\n",
      "[06/15/2025-10:08:37] [I] \n",
      "[06/15/2025-10:08:37] [I] === Inference Options ===\n",
      "[06/15/2025-10:08:37] [I] Batch: 1\n",
      "[06/15/2025-10:08:37] [I] Input inference shapes: model\n",
      "[06/15/2025-10:08:37] [I] Iterations: 100\n",
      "[06/15/2025-10:08:37] [I] Duration: 3s (+ 200ms warm up)\n",
      "[06/15/2025-10:08:37] [I] Sleep time: 0ms\n",
      "[06/15/2025-10:08:37] [I] Idle time: 0ms\n",
      "[06/15/2025-10:08:37] [I] Inference Streams: 1\n",
      "[06/15/2025-10:08:37] [I] ExposeDMA: Disabled\n",
      "[06/15/2025-10:08:37] [I] Data transfers: Enabled\n",
      "[06/15/2025-10:08:37] [I] Spin-wait: Disabled\n",
      "[06/15/2025-10:08:37] [I] Multithreading: Disabled\n",
      "[06/15/2025-10:08:37] [I] CUDA Graph: Disabled\n",
      "[06/15/2025-10:08:37] [I] Separate profiling: Disabled\n",
      "[06/15/2025-10:08:37] [I] Time Deserialize: Disabled\n",
      "[06/15/2025-10:08:37] [I] Time Refit: Disabled\n",
      "[06/15/2025-10:08:37] [I] NVTX verbosity: 0\n",
      "[06/15/2025-10:08:37] [I] Persistent Cache Ratio: 0\n",
      "[06/15/2025-10:08:37] [I] Inputs:\n",
      "[06/15/2025-10:08:37] [I] === Reporting Options ===\n",
      "[06/15/2025-10:08:37] [I] Verbose: Disabled\n",
      "[06/15/2025-10:08:37] [I] Averages: 10 inferences\n",
      "[06/15/2025-10:08:37] [I] Percentiles: 90,95,99\n",
      "[06/15/2025-10:08:37] [I] Dump refittable layers:Disabled\n",
      "[06/15/2025-10:08:37] [I] Dump output: Disabled\n",
      "[06/15/2025-10:08:37] [I] Profile: Disabled\n",
      "[06/15/2025-10:08:37] [I] Export timing to JSON file: \n",
      "[06/15/2025-10:08:37] [I] Export output to JSON file: \n",
      "[06/15/2025-10:08:37] [I] Export profile to JSON file: \n",
      "[06/15/2025-10:08:37] [I] \n",
      "[06/15/2025-10:08:37] [I] === Device Information ===\n",
      "[06/15/2025-10:08:37] [I] Selected Device: NVIDIA GeForce RTX 2060\n",
      "[06/15/2025-10:08:37] [I] Compute Capability: 7.5\n",
      "[06/15/2025-10:08:37] [I] SMs: 30\n",
      "[06/15/2025-10:08:37] [I] Device Global Memory: 6143 MiB\n",
      "[06/15/2025-10:08:37] [I] Shared Memory per SM: 64 KiB\n",
      "[06/15/2025-10:08:37] [I] Memory Bus Width: 192 bits (ECC disabled)\n",
      "[06/15/2025-10:08:37] [I] Application Compute Clock Rate: 1.71 GHz\n",
      "[06/15/2025-10:08:37] [I] Application Memory Clock Rate: 7.001 GHz\n",
      "[06/15/2025-10:08:37] [I] \n",
      "[06/15/2025-10:08:37] [I] Note: The application clock rates do not reflect the actual clock rates that the GPU is currently running at.\n",
      "[06/15/2025-10:08:37] [I] \n",
      "[06/15/2025-10:08:37] [I] TensorRT version: 8.6.3\n",
      "[06/15/2025-10:08:37] [I] Loading standard plugins\n",
      "[06/15/2025-10:08:37] [I] Engine loaded in 0.0471876 sec.\n",
      "[06/15/2025-10:08:38] [I] [TRT] Loaded engine size: 15 MiB\n",
      "[06/15/2025-10:08:38] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +14, now: CPU 0, GPU 14 (MiB)\n",
      "[06/15/2025-10:08:38] [I] Engine deserialized in 0.454017 sec.\n",
      "[06/15/2025-10:08:38] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +17, now: CPU 0, GPU 31 (MiB)\n",
      "[06/15/2025-10:08:38] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "[06/15/2025-10:08:38] [I] Setting persistentCacheLimit to 0 bytes.\n",
      "[06/15/2025-10:08:38] [I] Using random values for input images\n",
      "[06/15/2025-10:08:38] [I] Input binding for images with dimensions 1x3x640x640 is created.\n",
      "[06/15/2025-10:08:38] [I] Output binding for output0 with dimensions 1x25200x85 is created.\n",
      "[06/15/2025-10:08:38] [I] Starting inference\n",
      "[06/15/2025-10:08:41] [I] Warmup completed 67 queries over 200 ms\n",
      "[06/15/2025-10:08:41] [I] Timing trace has 1240 queries over 3.00694 s\n",
      "[06/15/2025-10:08:41] [I] \n",
      "[06/15/2025-10:08:41] [I] === Trace details ===\n",
      "[06/15/2025-10:08:41] [I] Trace averages of 10 runs:\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 2.324 ms - Host latency: 3.63464 ms (enqueue 0.307623 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 2.34462 ms - Host latency: 3.66319 ms (enqueue 0.314735 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.78658 ms - Host latency: 3.11984 ms (enqueue 0.295755 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.79136 ms - Host latency: 3.12597 ms (enqueue 0.305426 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.70172 ms - Host latency: 3.05453 ms (enqueue 0.29339 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74471 ms - Host latency: 3.12108 ms (enqueue 0.30065 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.71588 ms - Host latency: 3.02807 ms (enqueue 0.295062 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.73422 ms - Host latency: 3.1005 ms (enqueue 0.289511 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.7282 ms - Host latency: 3.07988 ms (enqueue 0.315628 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74602 ms - Host latency: 3.06181 ms (enqueue 0.322787 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.75715 ms - Host latency: 3.08798 ms (enqueue 0.358472 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.72491 ms - Host latency: 3.11185 ms (enqueue 0.367551 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.80323 ms - Host latency: 3.21046 ms (enqueue 0.399191 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.75662 ms - Host latency: 3.09971 ms (enqueue 0.296942 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.766 ms - Host latency: 3.17911 ms (enqueue 0.297778 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.73893 ms - Host latency: 3.08059 ms (enqueue 0.320087 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74781 ms - Host latency: 3.13438 ms (enqueue 0.295319 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74351 ms - Host latency: 3.09716 ms (enqueue 0.508081 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.75343 ms - Host latency: 3.11544 ms (enqueue 0.497485 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.7939 ms - Host latency: 3.57054 ms (enqueue 0.661426 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.77026 ms - Host latency: 3.22654 ms (enqueue 0.431311 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74023 ms - Host latency: 3.15408 ms (enqueue 0.435553 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.76717 ms - Host latency: 3.13124 ms (enqueue 0.389014 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.78921 ms - Host latency: 3.18488 ms (enqueue 0.335443 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.73861 ms - Host latency: 3.07067 ms (enqueue 0.291479 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.76745 ms - Host latency: 3.12253 ms (enqueue 0.333697 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.6905 ms - Host latency: 3.09365 ms (enqueue 0.304498 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.73797 ms - Host latency: 3.068 ms (enqueue 0.297528 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.7328 ms - Host latency: 3.10871 ms (enqueue 0.327997 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.75275 ms - Host latency: 3.10525 ms (enqueue 0.289801 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.69009 ms - Host latency: 3.04694 ms (enqueue 0.41275 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.73215 ms - Host latency: 3.06541 ms (enqueue 0.49743 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.75125 ms - Host latency: 3.05962 ms (enqueue 0.511469 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.70914 ms - Host latency: 3.0648 ms (enqueue 0.508051 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.78383 ms - Host latency: 3.17346 ms (enqueue 0.535669 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.77317 ms - Host latency: 3.09166 ms (enqueue 0.369421 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.78499 ms - Host latency: 3.31543 ms (enqueue 0.402527 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.71073 ms - Host latency: 3.03545 ms (enqueue 0.32356 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74318 ms - Host latency: 3.1364 ms (enqueue 0.29873 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.8116 ms - Host latency: 3.43351 ms (enqueue 0.451904 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.76289 ms - Host latency: 3.20295 ms (enqueue 0.367651 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.79141 ms - Host latency: 3.18448 ms (enqueue 0.380579 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.71077 ms - Host latency: 3.08325 ms (enqueue 0.297632 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.77034 ms - Host latency: 3.13331 ms (enqueue 0.318994 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.73954 ms - Host latency: 3.09608 ms (enqueue 0.308813 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74696 ms - Host latency: 3.09763 ms (enqueue 0.305493 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.71528 ms - Host latency: 3.08467 ms (enqueue 0.294556 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.77518 ms - Host latency: 3.11989 ms (enqueue 0.310559 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.76284 ms - Host latency: 3.11996 ms (enqueue 0.409094 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.7157 ms - Host latency: 3.1225 ms (enqueue 0.53501 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.78486 ms - Host latency: 3.1276 ms (enqueue 0.414587 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74753 ms - Host latency: 3.10848 ms (enqueue 0.384583 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.7408 ms - Host latency: 3.11918 ms (enqueue 0.369006 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74103 ms - Host latency: 3.07889 ms (enqueue 0.29436 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.78926 ms - Host latency: 3.19961 ms (enqueue 0.345117 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.7368 ms - Host latency: 3.06304 ms (enqueue 0.289587 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.81895 ms - Host latency: 3.34655 ms (enqueue 0.421973 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74691 ms - Host latency: 3.20018 ms (enqueue 0.339893 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.83182 ms - Host latency: 3.41631 ms (enqueue 0.388721 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74635 ms - Host latency: 3.10782 ms (enqueue 0.33197 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.75612 ms - Host latency: 3.10776 ms (enqueue 0.515381 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.67977 ms - Host latency: 3.05908 ms (enqueue 0.489429 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.80238 ms - Host latency: 3.2718 ms (enqueue 0.498621 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.76265 ms - Host latency: 3.18964 ms (enqueue 0.401904 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74626 ms - Host latency: 3.08506 ms (enqueue 0.311768 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74282 ms - Host latency: 3.07576 ms (enqueue 0.33562 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74856 ms - Host latency: 3.09985 ms (enqueue 0.306396 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.71215 ms - Host latency: 3.08121 ms (enqueue 0.327979 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.73782 ms - Host latency: 3.05364 ms (enqueue 0.293835 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.77886 ms - Host latency: 3.12046 ms (enqueue 0.329077 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.73497 ms - Host latency: 3.16738 ms (enqueue 0.335681 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.7775 ms - Host latency: 3.15475 ms (enqueue 0.304224 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.7272 ms - Host latency: 3.10215 ms (enqueue 0.284338 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.76244 ms - Host latency: 3.2483 ms (enqueue 0.345618 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.7371 ms - Host latency: 3.04926 ms (enqueue 0.298242 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.71102 ms - Host latency: 3.03024 ms (enqueue 0.292554 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74365 ms - Host latency: 3.14734 ms (enqueue 0.330371 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.7375 ms - Host latency: 3.06826 ms (enqueue 0.292383 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.80569 ms - Host latency: 3.1603 ms (enqueue 0.315308 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74272 ms - Host latency: 3.04814 ms (enqueue 0.30874 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.81533 ms - Host latency: 3.16213 ms (enqueue 0.34917 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.73931 ms - Host latency: 3.05203 ms (enqueue 0.33396 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.826 ms - Host latency: 3.28264 ms (enqueue 0.478931 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74358 ms - Host latency: 3.13411 ms (enqueue 0.466699 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.67983 ms - Host latency: 3.03623 ms (enqueue 0.292993 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.7509 ms - Host latency: 3.10313 ms (enqueue 0.322437 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.7365 ms - Host latency: 3.04573 ms (enqueue 0.289941 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.73645 ms - Host latency: 3.04895 ms (enqueue 0.291235 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.70933 ms - Host latency: 3.04932 ms (enqueue 0.289575 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74814 ms - Host latency: 3.17681 ms (enqueue 0.468164 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.71279 ms - Host latency: 3.08328 ms (enqueue 0.537012 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.77119 ms - Host latency: 3.14021 ms (enqueue 0.324756 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.73542 ms - Host latency: 3.0905 ms (enqueue 0.322168 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.73809 ms - Host latency: 3.04871 ms (enqueue 0.296899 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.77544 ms - Host latency: 3.07959 ms (enqueue 0.309448 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.70872 ms - Host latency: 3.05813 ms (enqueue 0.310962 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.78018 ms - Host latency: 3.16487 ms (enqueue 0.306055 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74761 ms - Host latency: 3.07878 ms (enqueue 0.323389 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.78159 ms - Host latency: 3.13325 ms (enqueue 0.290112 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.73276 ms - Host latency: 3.10073 ms (enqueue 0.319824 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.75754 ms - Host latency: 3.19771 ms (enqueue 0.300879 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.73613 ms - Host latency: 3.0896 ms (enqueue 0.317285 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.73992 ms - Host latency: 3.07817 ms (enqueue 0.300293 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.7811 ms - Host latency: 3.35317 ms (enqueue 0.369897 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.71914 ms - Host latency: 3.19163 ms (enqueue 0.330396 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.78625 ms - Host latency: 3.17314 ms (enqueue 0.353174 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.71201 ms - Host latency: 3.04033 ms (enqueue 0.306836 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.77219 ms - Host latency: 3.11636 ms (enqueue 0.332373 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.75071 ms - Host latency: 3.07495 ms (enqueue 0.369092 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.77314 ms - Host latency: 3.09341 ms (enqueue 0.48396 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74043 ms - Host latency: 3.05291 ms (enqueue 0.504126 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.75193 ms - Host latency: 3.13767 ms (enqueue 0.382471 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74258 ms - Host latency: 3.1408 ms (enqueue 0.336475 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.73645 ms - Host latency: 3.05715 ms (enqueue 0.304346 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.7417 ms - Host latency: 3.06973 ms (enqueue 0.29187 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.67917 ms - Host latency: 3.08508 ms (enqueue 0.295239 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.77942 ms - Host latency: 3.15896 ms (enqueue 0.29978 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.73904 ms - Host latency: 3.04949 ms (enqueue 0.29436 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.67959 ms - Host latency: 2.99214 ms (enqueue 0.28562 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.75891 ms - Host latency: 3.09238 ms (enqueue 0.291895 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.74465 ms - Host latency: 3.05198 ms (enqueue 0.288086 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.73269 ms - Host latency: 3.14946 ms (enqueue 0.306104 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.69983 ms - Host latency: 3.06221 ms (enqueue 0.340698 ms)\n",
      "[06/15/2025-10:08:41] [I] Average on 10 runs - GPU latency: 1.73169 ms - Host latency: 3.09819 ms (enqueue 0.319434 ms)\n",
      "[06/15/2025-10:08:41] [I] \n",
      "[06/15/2025-10:08:41] [I] === Performance summary ===\n",
      "[06/15/2025-10:08:41] [I] Throughput: 412.379 qps\n",
      "[06/15/2025-10:08:41] [I] Latency: min = 2.95032 ms, max = 4.2829 ms, mean = 3.13204 ms, median = 3.01599 ms, percentile(90%) = 3.47876 ms, percentile(95%) = 3.60303 ms, percentile(99%) = 3.95044 ms\n",
      "[06/15/2025-10:08:41] [I] Enqueue Time: min = 0.256104 ms, max = 0.991516 ms, mean = 0.352781 ms, median = 0.305603 ms, percentile(90%) = 0.52063 ms, percentile(95%) = 0.568848 ms, percentile(99%) = 0.695679 ms\n",
      "[06/15/2025-10:08:41] [I] H2D Latency: min = 0.393799 ms, max = 1.32019 ms, mean = 0.54122 ms, median = 0.621643 ms, percentile(90%) = 0.703125 ms, percentile(95%) = 0.770386 ms, percentile(99%) = 0.917969 ms\n",
      "[06/15/2025-10:08:41] [I] GPU Compute Time: min = 1.64526 ms, max = 2.95526 ms, mean = 1.75853 ms, median = 1.69305 ms, percentile(90%) = 2.01941 ms, percentile(95%) = 2.17497 ms, percentile(99%) = 2.38562 ms\n",
      "[06/15/2025-10:08:41] [I] D2H Latency: min = 0.650879 ms, max = 1.96228 ms, mean = 0.832295 ms, median = 0.900635 ms, percentile(90%) = 1.01953 ms, percentile(95%) = 1.1272 ms, percentile(99%) = 1.33197 ms\n",
      "[06/15/2025-10:08:41] [I] Total Host Walltime: 3.00694 s\n",
      "[06/15/2025-10:08:41] [I] Total GPU Compute Time: 2.18058 s\n",
      "[06/15/2025-10:08:41] [W] * GPU compute time is unstable, with coefficient of variance = 10.305%.\n",
      "[06/15/2025-10:08:41] [W]   If not already in use, locking GPU clock frequency or adding --useSpinWait may improve the stability.\n",
      "[06/15/2025-10:08:41] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[06/15/2025-10:08:41] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8603] # trtexec --loadEngine=yolov5s.engine --iterations=100\n"
     ]
    }
   ],
   "source": [
    "### 5️⃣ Benchmark TensorRT Engine\n",
    "# --loadEngine: Chạy inference benchmark trên engine đã build\n",
    "# --iterations=100: Chạy 100 lần để đánh giá tốc độ\n",
    "\n",
    "!trtexec --loadEngine=yolov5s.engine --iterations=100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ff18e0",
   "metadata": {},
   "source": [
    "# Cách 2: Dùng TensorRT Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337de3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import os\n",
    "\n",
    "FP16_MODE = True\n",
    "WORKSPACE_SIZE = 1 << 30  # 1GB\n",
    "\n",
    "# Dynamic shape configs\n",
    "MIN_BATCH = 1\n",
    "OPT_BATCH = 4\n",
    "MAX_BATCH = 16\n",
    "HEIGHT = 640\n",
    "WIDTH = 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5bd94877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/15/2025-10:15:38] [TRT] [I] The logger passed into createInferBuilder differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "[06/15/2025-10:15:38] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 1653, GPU 1265 (MiB)\n",
      "[06/15/2025-10:15:38] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n"
     ]
    }
   ],
   "source": [
    "# ==== STEP 1: Setup TensorRT logger & builder ==== #\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "builder = trt.Builder(TRT_LOGGER)\n",
    "network_flags = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "network = builder.create_network(network_flags)\n",
    "parser = trt.OnnxParser(network, TRT_LOGGER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124531b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/15/2025-10:15:39] [TRT] [W] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "✅ Parse ONNX thành công!\n"
     ]
    }
   ],
   "source": [
    "# ==== STEP 2: Parse ONNX model ==== #\n",
    "if not os.path.isfile(onnx_path):\n",
    "    raise FileNotFoundError(f\"Không tìm thấy file ONNX: {onnx_path}\")\n",
    "\n",
    "with open(onnx_path, 'rb') as f:\n",
    "    if not parser.parse(f.read()):\n",
    "        for idx in range(parser.num_errors):\n",
    "            print(parser.get_error(idx))\n",
    "        raise RuntimeError(\"❌ Không parse được ONNX!\")\n",
    "\n",
    "print(\"✅ Parse ONNX thành công!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "915106b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor: images, shape: (1, 3, 640, 640)\n",
      "Output tensor: output0, shape: (1, 25200, 85)\n"
     ]
    }
   ],
   "source": [
    "# ==== STEP 3: Xác định input/output model ==== #\n",
    "input_tensor = network.get_input(0)\n",
    "print(f\"Input tensor: {input_tensor.name}, shape: {input_tensor.shape}\")\n",
    "output_tensor = network.get_output(0)\n",
    "print(f\"Output tensor: {output_tensor.name}, shape: {output_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e890c405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43420/3744626081.py:3: DeprecationWarning: Use set_memory_pool_limit instead.\n",
      "  config.max_workspace_size = WORKSPACE_SIZE\n"
     ]
    }
   ],
   "source": [
    "# ==== STEP 4: Create builder config và set FP16 ==== #\n",
    "config = builder.create_builder_config()\n",
    "config.max_workspace_size = WORKSPACE_SIZE\n",
    "if FP16_MODE:\n",
    "    config.set_flag(trt.BuilderFlag.FP16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "09f80e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== STEP 5: Tạo Optimization Profile ==== #\n",
    "profile = builder.create_optimization_profile()\n",
    "input_name = input_tensor.name\n",
    "\n",
    "## dynamic shape input\n",
    "# profile.set_shape(input_name,\n",
    "#                   (MIN_BATCH, 3, HEIGHT, WIDTH),\n",
    "#                   (OPT_BATCH, 3, HEIGHT, WIDTH),\n",
    "#                   (MAX_BATCH, 3, HEIGHT, WIDTH))\n",
    "\n",
    "## static shape input\n",
    "profile.set_shape(input_name,\n",
    "                  (1, 3, HEIGHT, WIDTH),\n",
    "                  (1, 3, HEIGHT, WIDTH),\n",
    "                  (1, 3, HEIGHT, WIDTH))\n",
    "config.add_optimization_profile(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f314ea45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Bắt đầu build TensorRT engine...\n",
      "[06/15/2025-10:15:54] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[06/15/2025-10:15:54] [TRT] [I] Graph optimization time: 0.0303711 seconds.\n",
      "[06/15/2025-10:15:54] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
      "[06/15/2025-10:15:54] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43420/207055163.py:3: DeprecationWarning: Use build_serialized_network instead.\n",
      "  engine = builder.build_engine(network, config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/15/2025-10:21:07] [TRT] [I] Detected 1 inputs and 4 output network tensors.\n",
      "[06/15/2025-10:21:07] [TRT] [I] Total Host Persistent Memory: 290704\n",
      "[06/15/2025-10:21:07] [TRT] [I] Total Device Persistent Memory: 540672\n",
      "[06/15/2025-10:21:07] [TRT] [I] Total Scratch Memory: 512\n",
      "[06/15/2025-10:21:07] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 17 MiB, GPU 213 MiB\n",
      "[06/15/2025-10:21:07] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 108 steps to complete.\n",
      "[06/15/2025-10:21:07] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 4.05187ms to assign 11 blocks to 108 nodes requiring 17066496 bytes.\n",
      "[06/15/2025-10:21:07] [TRT] [I] Total Activation Memory: 17066496\n",
      "[06/15/2025-10:21:07] [TRT] [W] TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "[06/15/2025-10:21:07] [TRT] [W] If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "[06/15/2025-10:21:07] [TRT] [W] Check verbose logs for the list of affected weights.\n",
      "[06/15/2025-10:21:07] [TRT] [W] - 52 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "[06/15/2025-10:21:07] [TRT] [W] - 2 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n",
      "[06/15/2025-10:21:07] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +13, GPU +15, now: CPU 13, GPU 15 (MiB)\n"
     ]
    }
   ],
   "source": [
    "# ==== STEP 6: Build engine ==== #\n",
    "print(\"⚙️ Bắt đầu build TensorRT engine...\")\n",
    "engine = builder.build_engine(network, config)\n",
    "if engine is None:\n",
    "    raise RuntimeError(\"❌ Build engine thất bại!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938504af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Engine đã lưu: yolov5s_api.engine\n"
     ]
    }
   ],
   "source": [
    "# ==== STEP 7: Save engine to file ==== #\n",
    "with open(engine_path, 'wb') as f:\n",
    "    f.write(engine.serialize())\n",
    "print(f\"✅ Engine đã lưu: {engine_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4732d5f6",
   "metadata": {},
   "source": [
    "# 4️⃣ Code Thực Thi Inference TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed8e3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06/15/2025-15:23:09] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "Found 2 images in ./data/images\n",
      "Processed: ./data/images/bus.jpg -> output_results_trt/bus_output.jpg\n",
      "Processed: ./data/images/zidane.jpg -> output_results_trt/zidane_output.jpg\n"
     ]
    }
   ],
   "source": [
    "# Tạo 1 lệnh string chuẩn bằng f-string\n",
    "cmd = f\"python trt_infer.py --engine {engine_path} --input {img_folder_path} --output {output_folder_path}\"\n",
    "\n",
    "# Sau đó truyền vào shell bằng !\n",
    "!{cmd} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d224c8",
   "metadata": {},
   "source": [
    "# Code Thực Thi Inference ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "deef5875",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = \"yolov5s.onnx\"\n",
    "img_folder_path = './data/images'\n",
    "output_folder_path = 'output_results_onnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "695b92a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 images\n",
      "Saved ONNX output to output_results_onnx/bus_onnx.jpg\n",
      "Saved ONNX output to output_results_onnx/zidane_onnx.jpg\n"
     ]
    }
   ],
   "source": [
    "# Tạo 1 lệnh string chuẩn bằng f-string\n",
    "cmd = f\"python onnx_infer.py --onnx {onnx_path} --input {img_folder_path} --output {output_folder_path}\"\n",
    "\n",
    "# Sau đó truyền vào shell bằng !\n",
    "!{cmd} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0244a",
   "metadata": {},
   "source": [
    "# So sánh kết quả của onnx và engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe7bc676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Bắt đầu benchmark ===\n",
      "ONNX avg time: 62.121 ms\n",
      "[06/15/2025-11:27:19] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage and speed up TensorRT initialization. See \"Lazy Loading\" section of CUDA documentation https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#lazy-loading\n",
      "/app/tensorRt/yolo/tensorrt_benchmark.py:60: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  input_shape = engine.get_binding_shape(0)\n",
      "/app/tensorRt/yolo/tensorrt_benchmark.py:61: DeprecationWarning: Use get_tensor_shape instead.\n",
      "  output_shape = engine.get_binding_shape(1)\n",
      "TensorRT avg time: 4.687 ms\n",
      "TensorRT nhanh hơn 13.25 lần so với ONNX\n",
      "=== So sánh output ===\n",
      "Max difference: 4.368988037109375\n",
      "Mean difference: 0.0042354934848845005\n"
     ]
    }
   ],
   "source": [
    "!python tensorrt_benchmark.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
